{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c306ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing several libraries\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import sklearn\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba34051",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from nltk.classify.scikitlearn import SklearnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the CSV file\n",
    "data_path = 'Womens Clothing E-Commerce Reviews.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df=pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45654ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43559ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the dataset, rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e34cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique values in each column\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b0b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of nulls in each column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any row that contains at least one NaN value\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372417d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index after dropping some rows \n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164093ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the missing values after droping the null values \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46197617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary culomns \n",
    "df.drop([\"Unnamed: 0\", \"Title\", 'Clothing ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085b14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51590e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove spaces in columns and replace them with underscore \n",
    "df.columns= df.columns.str.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f4db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reviews Tuples to store the words along with the categories \n",
    "reviews = []\n",
    "# go through Recommended IND column and get the category and the index \n",
    "for (index , category) in enumerate(df.Recommended_IND):\n",
    "    reviews.append((df.Review_Text[index],category)) # Store the review for spacific index with catogory inside texts array\n",
    "# Print first 4\n",
    "reviews[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6878df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lemmatizer \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b60d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download necessary resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of stopwords \n",
    "# Get English stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "# Get a set of specific punctuation marks\n",
    "punctuations = set(string.punctuation)\n",
    "#print(punctuations)\n",
    "# Combine stopwords and punctuation sets\n",
    "stops.update(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d577e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total stopwords\n",
    "len(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f0e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the simpler version of pos tag  to use it in lemmitazation \n",
    "def get_simple_pos(tag):\n",
    "    tag_start = tag[0].upper()\n",
    "    if tag_start == 'N':\n",
    "        return wordnet.NOUN\n",
    "    elif tag_start == 'V':\n",
    "        return wordnet.VERB\n",
    "    elif tag_start == 'R':\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e1f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return Lemmatized words and cleaned from stop words\n",
    "def clean_review(words):\n",
    "    words_tokens = word_tokenize(words)\n",
    "    \n",
    "    output_words = [\n",
    "        lemmatizer.lemmatize(word.lower(), pos=get_simple_pos(tag))\n",
    "        for word, tag in pos_tag(words_tokens)\n",
    "        if word.lower() not in stops and any(c.isalnum() for c in word)\n",
    "    ]\n",
    "    \n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bacdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_words = clean_review(\"My cats are running away from my arms\")\n",
    "print(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews= [(clean_review(text),category )for text,category  in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a666b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy clean version into excel spreadsheet to be used for tableau purposes\n",
    "# Create a DataFrame from the list of tuples\n",
    "df = pd.DataFrame(cleaned_reviews, columns=['Cleaned_Text', 'Category'])\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('cleaned_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1536cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check first 5 reviews \n",
    "cleaned_reviews[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56759386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.75% traning = 14746 and 25% testing = 19662-14746 =4916 \n",
    "#traning_words=cleaned_reviews[0:14746]\n",
    "#testing_words=cleaned_reviews[14746:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d54186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets (e.g., 80% training, 20% testing)\n",
    "training_words, testing_words = train_test_split(cleaned_reviews, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Data = {len(training_words)}\")\n",
    "print(f\"Testing Data = {len(testing_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c4e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array contaning all words \n",
    "words_list=[]\n",
    "for word in training_words:\n",
    "        words_list+=word[0] # 0 index to get only the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f7e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total words in traning data \n",
    "len(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency distribution for all words \n",
    "freq= nltk.FreqDist(words_list)\n",
    "# The .most_common() method lists the words which occur most frequently in the data along with the frequency\n",
    "common=freq.most_common()\n",
    "# features are an array of only the top words in word list without The number of words \n",
    "features= [i[0]for i in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(common))\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common 5 words \n",
    "common[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a6826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of 5 features \n",
    "features[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c94ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the highest repeating words (features)\n",
    "\n",
    "# wordcload is techniqe use to show which words are the most frequent \n",
    "wordCloud = WordCloud(background_color=\"white\", max_words =3000).generate(str(features))\n",
    "\n",
    "rcParams[\"figure.figsize\"]= 10,20\n",
    "plt.imshow(wordCloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f80ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return a set of the features with true or false \n",
    "def get_dict_for_feature(words):\n",
    "  current_features={}\n",
    "  words_set= set(words)\n",
    "  for word in features:\n",
    "    current_features[word] = word in words_set  # if word comes in words set it will return True otherwise False \n",
    "  return current_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac64dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuers_dic= get_dict_for_feature(training_words[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6908d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary containing all words with True classification if the word is exist in each review otherwise false  \n",
    "featuers_dic  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30bd976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dic for each review wich has feature with value and the category\n",
    "training_words= [( get_dict_for_feature(words),category ) for words , category in training_words]\n",
    "testing_words = [( get_dict_for_feature(words),category ) for words , category in testing_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf58c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c843fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to the classifier we need to use NaiveBayesClassifier and pass the training words to it \n",
    "NB_classifier= NaiveBayesClassifier.train(training_words)\n",
    "print(\"classifier accuracy percent:\",(nltk.classify.accuracy(NB_classifier, training_words))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1 = \"Super fast and responsive with any issues. Different style print option was great! Easy to order and a pleasure to have done business with. Looking forward to ordering more items! Thank you\"\n",
    "review_2= \"I am thrilled with the quality & fit of the t-shirts& they were very nicely packaged too. I will definitely be re-ordering from you in the future. - Kristina - Spain\"\n",
    "review_3=\"Missing refunds. Returned parcel and got a date that I would get the refund by, five days after this date no refund. Contacted customer support and they advised I have to wait another 14 days. The service was very unhelpful and rude at times.\"\n",
    "reviews = [review_1,review_2,review_3]\n",
    "\n",
    "\n",
    "def test_custom_review(reviews_list, classifier):\n",
    "    \n",
    "    for idx,review in enumerate(reviews_list) : \n",
    "        custom_tokens = clean_review(review)\n",
    "        print(f\"The clean review is : \"  , str(custom_tokens).replace('[','').replace(']',''))\n",
    "        classifiers=classifier.classify(dict([token, True] for token in custom_tokens))\n",
    "        if (classifiers == 1):\n",
    "            pred = \"Positive\"\n",
    "        else:\n",
    "            pred = \"Negative\"\n",
    "        print(f\"Review number {idx +1 }  seems to be {pred} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d5c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_custom_review(reviews,NB_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51494ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
